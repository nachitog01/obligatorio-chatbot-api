# ğŸ§  Chatbot Inteligente â€“ Tienda Alemana

Este es el backend del proyecto obligatorio de Inteligencia Artificial para la materia, desarrollado con **FastAPI**, **LangChain** y **Ollama**. El sistema implementa una arquitectura **RAG (Retrieval-Augmented Generation)** que responde preguntas frecuentes sobre productos, gÃ³ndolas y sucursales de la Tienda Alemana, a partir de documentos PDF oficiales.

---

## ğŸ”— Repositorio GitHub

https://github.com/nachitog01/obligatorio-chatbot

---

## ğŸš€ Â¿QuÃ© hace este proyecto?

Procesa tres documentos PDF:

- CatÃ¡logo de productos (precios, stock)
- UbicaciÃ³n fÃ­sica de productos (secciones y gÃ³ndolas)
- InformaciÃ³n general de las sucursales (horarios, direcciones)

Permite hacer preguntas como:

- Â¿DÃ³nde encuentro el arroz de 1kg?
- Â¿CuÃ¡nto cuesta la leche entera?
- Â¿Hasta quÃ© hora abre la sucursal de Carrasco?

---

## âš™ï¸ TecnologÃ­as utilizadas

- **FastAPI** â€“ Web API backend  
- **LangChain** â€“ Encargado de manejar la arquitectura RAG  
- **ChromaDB** â€“ Vector store local para almacenamiento semÃ¡ntico  
- **Ollama** â€“ Modelo LLM local (`llama3.2`) para embeddings y respuestas  
- **dotenv** â€“ Para manejo de configuraciÃ³n  

---

## ğŸ§µ Â¿CÃ³mo se procesaron los PDFs?

- Se utilizÃ³ PyPDFLoader desde `langchain_community.document_loaders` para cargar los documentos.
- El contenido fue separado en chunks de tamaÃ±o 500 y solapamiento 50 usando RecursiveCharacterTextSplitter.
- Se utilizÃ³ Chroma como vector store para almacenar los embeddings.
- Los embeddings se generaron usando OllamaEmbeddings, configurado para usar el modelo local definido por `.env`.
- El pipeline se construyÃ³ con ConversationalRetrievalChain y el modelo se ejecuta vÃ­a `ollama`.

---

## ğŸ“ Estructura del proyecto

obligatorio-chatbot/  
â”œâ”€â”€ main.py                # API FastAPI  
â”œâ”€â”€ utils/  
â”‚   â”œâ”€â”€ pdf_loader.py  
â”‚   â”œâ”€â”€ vector_store.py  
â”‚   â””â”€â”€ rag_pipeline.py  
â”œâ”€â”€ data/  
â”‚   â”œâ”€â”€ Catalogo.pdf  
â”‚   â”œâ”€â”€ Ubicacion.pdf  
â”‚   â””â”€â”€ Locales.pdf  
â”œâ”€â”€ requirements.txt  
â”œâ”€â”€ .env.txt               # Plantilla de entorno  
â””â”€â”€ .gitignore  

---

## ğŸ“˜ Endpoints

### POST /ask

**Input:** JSON con una pregunta.  
Ejemplo:

{ "question": "Â¿DÃ³nde encuentro el yogur de frutilla?" }

**Output:** Respuesta generada por el modelo basada en los documentos cargados.

---

## ğŸ§ª CÃ³mo correr el proyecto localmente

1. Instalar dependencias:

    pip install -r requirements.txt

2. Crear el archivo `.env`:

    cp .env.txt .env

3. Revisar el contenido de `.env`:

    OLLAMA_MODEL=llama3.2  
    OLLAMA_URL=http://localhost:11434  
    CHUNK_SIZE=500  
    CHUNK_OVERLAP=50  

4. Correr Ollama con el modelo:

    ollama pull llama3.2  
    ollama run llama3.2  

5. Levantar el backend:

    python3 -m uvicorn main:app --reload

6. Probar desde Swagger:

    Visitar http://localhost:8000/docs

---

## ğŸ“Œ Notas

- El campo `chat_history` se pasa vacÃ­o, pero permite escalabilidad para mantener contexto en futuras versiones.
- El archivo `.env` real no se sube. Solo se entrega `.env.txt` como plantilla.
- El modelo debe estar corriendo localmente antes de iniciar la API.

---

## âœï¸ Autores

- Ignacio GarcÃ­a â€“ 255389  
- Violeta Clerc â€“ 233379  
- Ignacio Azaretto â€“ 243242
